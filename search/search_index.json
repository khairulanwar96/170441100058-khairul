{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"K-Means Clustering Pengertian Algoritma K-Means Clustering \u200b K-Means Clustering adalah salah satu algoritma dalam menentukan klasifikasi terhadap objek berdasarkan attribut / fitur dari objek tersebut kedalam K kluster/partisi. K adalah angka positif yang menyatakan jumlah grup/kluster/partisi terhadap objek. Pemartisian data dilakukan dengan mencari nilai jarak minimum antara data dan nilai centroid yang telah di set baik secara random atau pun dengan Initial Set of Centroids, kita juga dapat menentukan nilai centroid berdasarkan K object yang berurutan. Algoritma Metode K Means Clustering Algoritma Flowchart Metode Klastering K Means Langkah- Langkah perhitungannya adalah: Menentukan Jumlah cluster data Tentukan titik pusat cluster menghitung jarak obyek dengan centroid kelompokan obyek jika kelompok data hasil perhitungan baru sama dengan hasil perhitungan kelompok data baru maka selesailah perhitungannya. Contoh K-Means Clustering dataset padi. Dalam tahap ini akan dijelaskan langkah-langkah pengoperasian algoritma K-Means secara manual: Diketahui : Jumlah Cluster = 3, jumlah data =12, jumlah atribut= 2 NO Kota /Kab Luas Lahan Produksi 1 Kab. Ponorogo 66,693.00 402,047.00 2 Kab. Trenggalek 31,136.00 182,848.00 3 Kab. Tulungagung 49,230.00 259,581.00 4 Kab. Blitar 50,577.00 289,494.00 5 Kab. Kediri 51,083.00 281,392.00 6 Kab. Malang 65,597.00 464,498.00 7 Kab. Lumajang 72,552.00 387,168.00 8 Kab. Jember 162,619.00 964,001.00 9 Kab. Banyuwangi 113,609.00 706,419.00 10 Kab. Bondowoso 61,330.00 329,557.00 11 Kab. Situbondo 48,902.00 290,954.00 12 Kab. Probolinggo 59,130.00 311,258.00 Tabel 2 : Tabel Sampel Dataset Padi tahun 2013 Provinsi Jawa Timur Iterasi ke-1 1. Penentuan pusat awal cluster Di ambil data ke-8 sebagai pusat cluster ke-1 162,619 964,001 Di ambil data ke-7 sebagai pusat cluster ke-2 72,552 387,168 Di ambil data ke-2 sebagai pusat cluster ke-3 31,136 182,848 2. Perhitungan Jarak Pusat Cluster Untuk mengukur jarak antara dengan pusat Cluster digunakan Euclidian Distance , kemudian akan didapatkan matriks jarak yaitu C1, C2 dan C3 sebagai berikut: Rumus Euclidian Distance: NO Kota /Kab Luas Lahan Produksi C1 C2 C3 Jarak Terpendek 1 Kab. Ponorogo 66,693 402,047 570,082534 15,99101 222,0642 15,99101379 2 Kab. Trenggalek 31,136 182,848 792,1412681 208,4753 0 0 3 Kab. Tulungagung 49,23 259,581 713,4876325 129,701 78,83747 78,83746651 4 Kab. Blitar 50,577 289,494 683,7492982 100,1155 108,4035 100,1154878 5 Kab. Kediri 51,083 281,392 691,6612799 107,9328 100,5425 100,542542 6 Kab. Malang 65,597 464,498 508,8383982 77,64213 283,7504 77,6421337 7 Kab. Lumajang 72,552 387,168 583,8222113 0 208,4753 0 8 Kab. Jember 162,619 964,001 0 583,8222 792,1413 0 9 Kab. Banyuwangi 113,609 706,419 262,2031022 321,8802 530,0268 262,2031022 10 Kab. Bondowoso 61,33 329,557 642,4785216 58,69379 149,7839 58,69378677 11 Kab. Situbondo 48,902 290,954 682,5861267 99,07803 109,5561 99,07803135 12 Kab. Probolinggo 59,13 311,258 660,8959049 77,08747 131,426 77,08747099 3. Pengelompokan Data Jarak hasil perhitungan akan dilakukan perbandingan dan dipilih jarak terdekat antara data dengan pusat cluster, jarak ini menunjukkan bahwa data tersebut berada dalam satu kelompok dengan pusat cluster terdekat. Berikut ini akan ditampilkan data matriks pengelompokkan group, nilai 1 berarti data tersebut berada dalam group(kelompok data). (Kelompok Data 1) No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 4. Penentuan pusat cluster baru Setelah diketahui anggota tiap-tiap cluster kemudian pusat cluster baru dihitung berdasarkan data anggota tiap-tiap cluster sesuai dengan rumus pusat anggota cluster . Sehingga didapatkan perhitungan sebagai berikut : NO Kota /Kab Luas Lahan Produksi Cluster Baru C1 C2 C3 1 Kab. Ponorogo 66,693 402,047 138,114 60,683 43,81633 2 Kab. Trenggalek 31,136 182,848 835,21 353,568 241,2737 3 Kab. Tulungagung 49,23 259,581 4 Kab. Blitar 50,577 289,494 5 Kab. Kediri 51,083 281,392 6 Kab. Malang 65,597 464,498 7 Kab. Lumajang 72,552 387,168 8 Kab. Jember 162,619 964,001 9 Kab. Banyuwangi 113,609 706,419 10 Kab. Bondowoso 61,33 329,557 11 Kab. Situbondo 48,902 290,954 12 Kab. Probolinggo 59,13 311,258 Iterasi ke-2 5. Ulangi langkah ke 2 (kedua) hingga posisi data tidak mengalami perubahan. Cluster baru yang ke-1 138,114 835,21 Cluster baru yang ke-2 60,683 353,568 Cluster baru yang ke-3 43,81633333 241,2736667 NO Kota /Kab Luas Lahan Produksi C1 C2 C3 Jarak Terpendek 1 Kab. Ponorogo 66,693 402,047 439,0116 48,85011 162,3928 48,85011301 2 Kab. Trenggalek 31,136 182,848 661,0752 173,258 59,78586 59,7858627 3 Kab. Tulungagung 49,23 259,581 582,451 94,68224 19,091 19,09099894 4 Kab. Blitar 50,577 289,494 552,6922 64,86608 48,69196 48,69196197 5 Kab. Kediri 51,083 281,392 560,6146 72,81164 40,77113 40,77113089 6 Kab. Malang 65,597 464,498 377,7381 111,0388 224,2844 111,0387873 7 Kab. Lumajang 72,552 387,168 452,8134 35,63472 148,6973 35,63471848 8 Kab. Jember 162,619 964,001 131,1016 618,8856 732,4267 131,1015511 9 Kab. Banyuwangi 113,609 706,419 131,1016 356,7982 470,3522 131,1015511 10 Kab. Bondowoso 61,33 329,557 511,4496 24,01972 90,00375 24,01971544 11 Kab. Situbondo 48,902 290,954 551,5192 63,71268 49,93996 49,93995921 12 Kab. Probolinggo 59,13 311,258 529,8718 42,33849 71,64018 42,33849205 Langkah selanjutnya sama dengan langkah pada nomor 3 jarak hasil perhitungan akan dilakukan perbandingan dan dipilih jarak terdekat antara data dengan pusat cluster , jarak ini menunjukkan bahwa data tersebut berada dalam satu kelompok dengan pusat cluster terdekat. Kelompok Data 2 No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 Pada perhitungan ini Iterasi Berhenti pada iterasi ke-4 karena kelompok data 4 = kelompok data 3 dan hasil Clustering, telah mencapai stabil dan konvergen. Untuk perhitungan yang lebih rinci di lampirkan pada file Excel. No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 Tabel Kelompok data 3 No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 Tabel Kelompok Data 4 referensi : https://syafrudinmtop.blogspot.com/2015/10/contoh-perhitungan-manual-kmeans-klastering.html https://ekojunaidisalam.com/2017/02/09/k-means-clustering-algorithm/","title":"K-Means"},{"location":"#k-means-clustering","text":"","title":"K-Means Clustering"},{"location":"#pengertian-algoritma-k-means-clustering","text":"\u200b K-Means Clustering adalah salah satu algoritma dalam menentukan klasifikasi terhadap objek berdasarkan attribut / fitur dari objek tersebut kedalam K kluster/partisi. K adalah angka positif yang menyatakan jumlah grup/kluster/partisi terhadap objek. Pemartisian data dilakukan dengan mencari nilai jarak minimum antara data dan nilai centroid yang telah di set baik secara random atau pun dengan Initial Set of Centroids, kita juga dapat menentukan nilai centroid berdasarkan K object yang berurutan.","title":"Pengertian Algoritma K-Means Clustering"},{"location":"#algoritma-metode-k-means-clustering","text":"Algoritma Flowchart Metode Klastering K Means Langkah- Langkah perhitungannya adalah: Menentukan Jumlah cluster data Tentukan titik pusat cluster menghitung jarak obyek dengan centroid kelompokan obyek jika kelompok data hasil perhitungan baru sama dengan hasil perhitungan kelompok data baru maka selesailah perhitungannya.","title":"Algoritma Metode K Means Clustering"},{"location":"#contoh-k-means-clustering-dataset-padi","text":"Dalam tahap ini akan dijelaskan langkah-langkah pengoperasian algoritma K-Means secara manual: Diketahui : Jumlah Cluster = 3, jumlah data =12, jumlah atribut= 2 NO Kota /Kab Luas Lahan Produksi 1 Kab. Ponorogo 66,693.00 402,047.00 2 Kab. Trenggalek 31,136.00 182,848.00 3 Kab. Tulungagung 49,230.00 259,581.00 4 Kab. Blitar 50,577.00 289,494.00 5 Kab. Kediri 51,083.00 281,392.00 6 Kab. Malang 65,597.00 464,498.00 7 Kab. Lumajang 72,552.00 387,168.00 8 Kab. Jember 162,619.00 964,001.00 9 Kab. Banyuwangi 113,609.00 706,419.00 10 Kab. Bondowoso 61,330.00 329,557.00 11 Kab. Situbondo 48,902.00 290,954.00 12 Kab. Probolinggo 59,130.00 311,258.00 Tabel 2 : Tabel Sampel Dataset Padi tahun 2013 Provinsi Jawa Timur Iterasi ke-1 1. Penentuan pusat awal cluster Di ambil data ke-8 sebagai pusat cluster ke-1 162,619 964,001 Di ambil data ke-7 sebagai pusat cluster ke-2 72,552 387,168 Di ambil data ke-2 sebagai pusat cluster ke-3 31,136 182,848 2. Perhitungan Jarak Pusat Cluster Untuk mengukur jarak antara dengan pusat Cluster digunakan Euclidian Distance , kemudian akan didapatkan matriks jarak yaitu C1, C2 dan C3 sebagai berikut: Rumus Euclidian Distance: NO Kota /Kab Luas Lahan Produksi C1 C2 C3 Jarak Terpendek 1 Kab. Ponorogo 66,693 402,047 570,082534 15,99101 222,0642 15,99101379 2 Kab. Trenggalek 31,136 182,848 792,1412681 208,4753 0 0 3 Kab. Tulungagung 49,23 259,581 713,4876325 129,701 78,83747 78,83746651 4 Kab. Blitar 50,577 289,494 683,7492982 100,1155 108,4035 100,1154878 5 Kab. Kediri 51,083 281,392 691,6612799 107,9328 100,5425 100,542542 6 Kab. Malang 65,597 464,498 508,8383982 77,64213 283,7504 77,6421337 7 Kab. Lumajang 72,552 387,168 583,8222113 0 208,4753 0 8 Kab. Jember 162,619 964,001 0 583,8222 792,1413 0 9 Kab. Banyuwangi 113,609 706,419 262,2031022 321,8802 530,0268 262,2031022 10 Kab. Bondowoso 61,33 329,557 642,4785216 58,69379 149,7839 58,69378677 11 Kab. Situbondo 48,902 290,954 682,5861267 99,07803 109,5561 99,07803135 12 Kab. Probolinggo 59,13 311,258 660,8959049 77,08747 131,426 77,08747099 3. Pengelompokan Data Jarak hasil perhitungan akan dilakukan perbandingan dan dipilih jarak terdekat antara data dengan pusat cluster, jarak ini menunjukkan bahwa data tersebut berada dalam satu kelompok dengan pusat cluster terdekat. Berikut ini akan ditampilkan data matriks pengelompokkan group, nilai 1 berarti data tersebut berada dalam group(kelompok data). (Kelompok Data 1) No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 4. Penentuan pusat cluster baru Setelah diketahui anggota tiap-tiap cluster kemudian pusat cluster baru dihitung berdasarkan data anggota tiap-tiap cluster sesuai dengan rumus pusat anggota cluster . Sehingga didapatkan perhitungan sebagai berikut : NO Kota /Kab Luas Lahan Produksi Cluster Baru C1 C2 C3 1 Kab. Ponorogo 66,693 402,047 138,114 60,683 43,81633 2 Kab. Trenggalek 31,136 182,848 835,21 353,568 241,2737 3 Kab. Tulungagung 49,23 259,581 4 Kab. Blitar 50,577 289,494 5 Kab. Kediri 51,083 281,392 6 Kab. Malang 65,597 464,498 7 Kab. Lumajang 72,552 387,168 8 Kab. Jember 162,619 964,001 9 Kab. Banyuwangi 113,609 706,419 10 Kab. Bondowoso 61,33 329,557 11 Kab. Situbondo 48,902 290,954 12 Kab. Probolinggo 59,13 311,258 Iterasi ke-2 5. Ulangi langkah ke 2 (kedua) hingga posisi data tidak mengalami perubahan. Cluster baru yang ke-1 138,114 835,21 Cluster baru yang ke-2 60,683 353,568 Cluster baru yang ke-3 43,81633333 241,2736667 NO Kota /Kab Luas Lahan Produksi C1 C2 C3 Jarak Terpendek 1 Kab. Ponorogo 66,693 402,047 439,0116 48,85011 162,3928 48,85011301 2 Kab. Trenggalek 31,136 182,848 661,0752 173,258 59,78586 59,7858627 3 Kab. Tulungagung 49,23 259,581 582,451 94,68224 19,091 19,09099894 4 Kab. Blitar 50,577 289,494 552,6922 64,86608 48,69196 48,69196197 5 Kab. Kediri 51,083 281,392 560,6146 72,81164 40,77113 40,77113089 6 Kab. Malang 65,597 464,498 377,7381 111,0388 224,2844 111,0387873 7 Kab. Lumajang 72,552 387,168 452,8134 35,63472 148,6973 35,63471848 8 Kab. Jember 162,619 964,001 131,1016 618,8856 732,4267 131,1015511 9 Kab. Banyuwangi 113,609 706,419 131,1016 356,7982 470,3522 131,1015511 10 Kab. Bondowoso 61,33 329,557 511,4496 24,01972 90,00375 24,01971544 11 Kab. Situbondo 48,902 290,954 551,5192 63,71268 49,93996 49,93995921 12 Kab. Probolinggo 59,13 311,258 529,8718 42,33849 71,64018 42,33849205 Langkah selanjutnya sama dengan langkah pada nomor 3 jarak hasil perhitungan akan dilakukan perbandingan dan dipilih jarak terdekat antara data dengan pusat cluster , jarak ini menunjukkan bahwa data tersebut berada dalam satu kelompok dengan pusat cluster terdekat. Kelompok Data 2 No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 Pada perhitungan ini Iterasi Berhenti pada iterasi ke-4 karena kelompok data 4 = kelompok data 3 dan hasil Clustering, telah mencapai stabil dan konvergen. Untuk perhitungan yang lebih rinci di lampirkan pada file Excel. No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 Tabel Kelompok data 3 No. C1 C2 C3 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 1 10 1 11 1 12 1 Tabel Kelompok Data 4 referensi : https://syafrudinmtop.blogspot.com/2015/10/contoh-perhitungan-manual-kmeans-klastering.html https://ekojunaidisalam.com/2017/02/09/k-means-clustering-algorithm/","title":"Contoh K-Means Clustering dataset padi."},{"location":"decision/","text":"Mengenal Decision Tree dan Manfaatnya Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree (pohon keputusan). Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Classfication Tree. Credit: Pronoz Untuk gambar diatas merupakan contoh dari classification tree, sedangkan gambar dibawah merupakan contoh dari regression tree. Regression Tree. Credit: Brandewinder.com Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan dalam metode ini, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja. Decision Tree atau CART. Credit: Towards Data Science Kelebihan lain dari metode ini adalah mampu mengeliminasi perhitungan atau data-data yang kiranya tidak diperlukan. Sebab, sampel yang ada biasanya hanya diuji berdasarkan kriteria atau kelas tertentu saja. Meski memiliki banyak kelebihan, namun bukan berarti metode ini tidak memiliki kekurangan. Decision tree ini bisa terjadi overlap, terutama ketika kelas dan kriteria yang digunakan sangat banyak tentu saja dapat meningkatkan waktu pengambilan keputusan sesuai dengan jumlah memori yang dibutuhkan. Dalam hal akumulasi, decision tree juga seringkali mengalami kendala eror terutama dalam jumlah besar. Selain itu, terdapat pula kesulitan dalam mendesain decision tree yang optimal. Apalagi mengingat kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. Terlepas dari kekurangan dan kelebihan dari decision tree , metode ini banyak digunakan lebih lanjut dalam berbagai pengolahan data. Mulai dari data mining dan juga machine learning . Dalam dunia kerja, decision tree sendiri sangat berguna untuk penilaian credit scoring. Jika anda pernah mengajukan kredit yang diproses secara instan, nah anda sudah mempunyai pengalaman dari decision tree . Implemetasi Decision Tree 'Dataset Titanic' # This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the \"../input/\" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory import os print(os.listdir(\"../input\")) # Any results you write to the current directory are saved as output. Titanic dengan pohon keputusan Menggunakan DecisionTreeClassifier sklearn untuk membuat gambar pohon untuk selamat dari titanic in [2] import numpy as np from sklearn.tree import DecisionTreeClassifier, export_graphviz from sklearn.model_selection import train_test_split import graphviz from subprocess import check_call from IPython.display import Image as PImage from PIL import Image, ImageDraw, ImageFont df = pd.read_csv('../input/train.csv', error_bad_lines=False) df.head(3) out [2] PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S Sebelum kita melanjutkan, mari kita lakukan sedikit persiapan data. Mari kita lepaskan NaN apa pun - ada opsi lain tentu saja untuk NaN, tetapi kami tertarik untuk membuat pohon keputusan yang tidak membuat penggolong paling akurat yang pernah ada df = df.dropna(axis=1) df = df.drop(['Name', 'Ticket'], axis=1) Sekarang pohon keputusan yang akan kita gunakan tidak bekerja dengan baik dengan data kategorikal, jadi mari kita gunakan fungsi get_dummies panda untuk membuat satu pengodean panas dari kolom yang merepotkan Kami juga akan membagi data menjadi set pelatihan dan tes X = pd.get_dummies(df.drop('Survived', axis=1)) Y = pd.get_dummies(df['Survived']) X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42) Hanya karena ketertarikan, ini adalah bagaimana get_dummies mengubah data in [5] X_train.head(3) out [5] PassengerId Pclass SibSp Parch Fare Sex_female Sex_male 6 7 1 0 0 51.8625 0 1 718 719 3 0 0 15.5000 0 1 685 686 2 1 2 41.5792 0 1 Jadi mari kita latih classifier! Seperti yang Anda lihat di bawah ini, kami telah menetapkan max_depth = 3 dan min_samples_split = 20 untuk classifier ini. Itu untuk mencegah overfitting clf = DecisionTreeClassifier(max_depth=3, min_samples_split=20) clf = clf.fit(X_train, y_train) in [7] clf.score(X_test, y_test) out [7] 0.8135593220338984 in [8] with open(\"tree1.dot\", 'w') as f: f = export_graphviz(clf, out_file=f, max_depth = 3, impurity = True, feature_names = list(X_train), class_names = ['Died', 'Survived'], rounded = True, filled= True ) check_call(['dot','-Tpng','tree1.dot','-o','tree1.png']) out [8] 0 in [9] img = Image.open(\"tree1.png\") draw = ImageDraw.Draw(img) img.save('sample-out.png') PImage(\"sample-out.png\") out [9] Referensi : https://medium.com/iykra/mengenal-decision-tree-dan-manfaatnya-b98cf3cf6a8d https://www.kaggle.com/hamishdickson/playing-with-decision-tree-classifiers/data","title":"Decision Tree"},{"location":"decision/#mengenal-decision-tree-dan-manfaatnya","text":"Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree (pohon keputusan). Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Classfication Tree. Credit: Pronoz Untuk gambar diatas merupakan contoh dari classification tree, sedangkan gambar dibawah merupakan contoh dari regression tree. Regression Tree. Credit: Brandewinder.com Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan dalam metode ini, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja. Decision Tree atau CART. Credit: Towards Data Science Kelebihan lain dari metode ini adalah mampu mengeliminasi perhitungan atau data-data yang kiranya tidak diperlukan. Sebab, sampel yang ada biasanya hanya diuji berdasarkan kriteria atau kelas tertentu saja. Meski memiliki banyak kelebihan, namun bukan berarti metode ini tidak memiliki kekurangan. Decision tree ini bisa terjadi overlap, terutama ketika kelas dan kriteria yang digunakan sangat banyak tentu saja dapat meningkatkan waktu pengambilan keputusan sesuai dengan jumlah memori yang dibutuhkan. Dalam hal akumulasi, decision tree juga seringkali mengalami kendala eror terutama dalam jumlah besar. Selain itu, terdapat pula kesulitan dalam mendesain decision tree yang optimal. Apalagi mengingat kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. Terlepas dari kekurangan dan kelebihan dari decision tree , metode ini banyak digunakan lebih lanjut dalam berbagai pengolahan data. Mulai dari data mining dan juga machine learning . Dalam dunia kerja, decision tree sendiri sangat berguna untuk penilaian credit scoring. Jika anda pernah mengajukan kredit yang diproses secara instan, nah anda sudah mempunyai pengalaman dari decision tree .","title":"Mengenal Decision Tree dan Manfaatnya"},{"location":"decision/#implemetasi-decision-tree","text":"","title":"Implemetasi Decision Tree"},{"location":"decision/#dataset-titanic","text":"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) # Input data files are available in the \"../input/\" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory import os print(os.listdir(\"../input\")) # Any results you write to the current directory are saved as output.","title":"'Dataset Titanic'"},{"location":"decision/#titanic-dengan-pohon-keputusan","text":"Menggunakan DecisionTreeClassifier sklearn untuk membuat gambar pohon untuk selamat dari titanic","title":"Titanic dengan pohon keputusan"},{"location":"decision/#in-2","text":"import numpy as np from sklearn.tree import DecisionTreeClassifier, export_graphviz from sklearn.model_selection import train_test_split import graphviz from subprocess import check_call from IPython.display import Image as PImage from PIL import Image, ImageDraw, ImageFont df = pd.read_csv('../input/train.csv', error_bad_lines=False) df.head(3)","title":"in [2]"},{"location":"decision/#out-2","text":"PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S Sebelum kita melanjutkan, mari kita lakukan sedikit persiapan data. Mari kita lepaskan NaN apa pun - ada opsi lain tentu saja untuk NaN, tetapi kami tertarik untuk membuat pohon keputusan yang tidak membuat penggolong paling akurat yang pernah ada df = df.dropna(axis=1) df = df.drop(['Name', 'Ticket'], axis=1) Sekarang pohon keputusan yang akan kita gunakan tidak bekerja dengan baik dengan data kategorikal, jadi mari kita gunakan fungsi get_dummies panda untuk membuat satu pengodean panas dari kolom yang merepotkan Kami juga akan membagi data menjadi set pelatihan dan tes X = pd.get_dummies(df.drop('Survived', axis=1)) Y = pd.get_dummies(df['Survived']) X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42) Hanya karena ketertarikan, ini adalah bagaimana get_dummies mengubah data","title":"out [2]"},{"location":"decision/#in-5","text":"X_train.head(3)","title":"in [5]"},{"location":"decision/#out-5","text":"PassengerId Pclass SibSp Parch Fare Sex_female Sex_male 6 7 1 0 0 51.8625 0 1 718 719 3 0 0 15.5000 0 1 685 686 2 1 2 41.5792 0 1 Jadi mari kita latih classifier! Seperti yang Anda lihat di bawah ini, kami telah menetapkan max_depth = 3 dan min_samples_split = 20 untuk classifier ini. Itu untuk mencegah overfitting clf = DecisionTreeClassifier(max_depth=3, min_samples_split=20) clf = clf.fit(X_train, y_train)","title":"out [5]"},{"location":"decision/#in-7","text":"clf.score(X_test, y_test)","title":"in [7]"},{"location":"decision/#out-7","text":"0.8135593220338984","title":"out [7]"},{"location":"decision/#in-8","text":"with open(\"tree1.dot\", 'w') as f: f = export_graphviz(clf, out_file=f, max_depth = 3, impurity = True, feature_names = list(X_train), class_names = ['Died', 'Survived'], rounded = True, filled= True ) check_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])","title":"in [8]"},{"location":"decision/#out-8","text":"0","title":"out [8]"},{"location":"decision/#in-9","text":"img = Image.open(\"tree1.png\") draw = ImageDraw.Draw(img) img.save('sample-out.png') PImage(\"sample-out.png\")","title":"in [9]"},{"location":"decision/#out-9","text":"Referensi : https://medium.com/iykra/mengenal-decision-tree-dan-manfaatnya-b98cf3cf6a8d https://www.kaggle.com/hamishdickson/playing-with-decision-tree-classifiers/data","title":"out [9]"},{"location":"knn/","text":"Pengertian KNN(k-nearest neighbor) Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik. Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya: Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu). Implementasi KNN Pada data \"Male Female Detection By Height Weight\". Mengimpor dan mengumpulkan data data=pd.read_csv('../input/weight-height.csv') data.head(10)import numpy as np from collections import Counter import matplotlib.pyplot as plt import pandas as pd data=pd.read_csv('../input/weight-height.csv') data.head(10) Menghapus kolom \"gender\" yang untuk menampilkan data numerik df=data columns=['Gender'] df = df.drop(columns, axis=1) df.head(10) Menggabungkan string \"frame\" dan di tampung menggunakan array Xm=df[0:25] Xf=df[5001:5026] frame=[Xm,Xf] X_train=pd.concat(frame) X_train=np.array(X_train) print(X_train) X_train.shape lbl=data['Gender'] ym=lbl[0:25] yf=lbl[5001:5026] frames=[ym,yf] Y_train=pd.concat(frames) Y_train=np.array(Y_train) print(Y_train) Y_train.shape label=data['Gender'] lb=label[400:410] lc=label[9000:9010] frames=[lb,lc] Y=pd.concat(frames) Y=np.array(Y) print(Y) Menampikan akurasi data \"gender\" x = data['Height'] y=data['Weight'] male=x[0:25] female=x[5001:5026] wm=y[0:25] wf=y[5001:5026] plt.scatter(male,wm, color='b') plt.scatter(female,wf,color='g') plt.xlabel('Male') plt.ylabel('Female') plt.show() Menggabungkan string \"frame\" dan di tampung menggunakan array testx=df[400:410] testy=df[9000:9010] frames=[testx,testy] X=pd.concat(frames) X=np.array(X) print(X) X.shape Membuat fungsi sederhana dengan beberapa argument secara anonymous def predict(input_feature_set, k): distances = [] z=0 for training_feature_set in X_train: group=Y_train[z] #print(\"Group=\",group) #print(\"Training Feature=\",training_feature_set) euclidean_distance = np.linalg.norm(np.array(input_feature_set) - np.array(training_feature_set)) #print(\"Distance=\",euclidean_distance) distances.append([euclidean_distance, group]) z=z+1 #print(z) nearest = sorted(distances)[:k] #print(\"Sorted=\",nearest) votes=[] #votes = [d[1] for d in nearest] for d in nearest: votes.append(d[1]) #print(votes) #prediction = Counter(votes).most_common(1)[0][0] item={} for i in votes: if i in item: item[i]=item[i]+1 else: item[i]=1 #finding most common class m=0 for k in item: if item[k] m: m=item[k] for k in item: if item[k]==m: index=k prediction=index return prediction Menampilkan data training \"Gender\" #Training toutput=[] for j in X_train: predicted=predict(j,3) toutput.append(predicted) print(\"Output=\",toutput) print(len(toutput)) Menampilkan training acuracy c=0 for p in range(0,50): if toutput[p]==Y_train[p]: c=c+1 result=(c/50.0)*100 print(\"Training Accuracy=\",result,\"%\") Menampilkan prediksi pada data \"Gender\" output=[] for j in X: predicted=predict(j,3) output.append(predicted) print(\"Output=\",output) Menampilkan Testing acuracy c=0 for p in range(0,20): if output[p]==Y[p]: c=c+1 result=(c/20.0)*100 print(\"Testing Accuracy=\",result,\"%\") Hasil Implementasi [[ 73.84701702 241.89356318] [ 68.78190405 162.31047252] [ 74.11010539 212.74085556] [ 71.7309784 220.0424703 ] [ 69.88179586 206.34980062] [ 67.25301569 152.21215576] [ 68.78508125 183.9278886 ] [ 68.34851551 167.97111049] [ 67.01894966 175.9294404 ] [ 63.45649398 156.39967639] [ 71.19538228 186.60492556] [ 71.64080512 213.74116949] [ 64.76632913 167.12746107] [ 69.2830701 189.44618139] [ 69.24373223 186.43416802] [ 67.6456197 172.18693006] [ 72.41831663 196.02850633] [ 63.97432572 172.88347021] [ 69.6400599 185.98395757] [ 67.93600485 182.42664801] [ 67.91505019 174.11592908] [ 69.43943987 197.73142161] [ 66.14913196 149.17356601] [ 75.20597361 228.76178062] [ 67.89319634 162.00665185] [ 65.23001251 141.3058226 ] [ 63.36900376 131.04140269] [ 64.47999743 128.17151122] [ 61.79309615 129.78140705] [ 65.96801895 156.80208261] [ 62.85037864 114.96903825] [ 65.65215644 165.08300121] [ 61.89023374 111.67619921] [ 63.67786815 104.15155964] [ 68.10117224 166.57566076] [ 61.79887853 106.23368699] [ 63.37145896 128.11816912] [ 58.89588635 101.68261336] [ 58.4382491 98.19262093] [ 60.80979868 126.91546328] [ 70.12865283 151.25427035] [ 62.25742965 115.79739341] [ 61.73509022 107.86687236] [ 63.05955669 145.58992915] [ 62.28683837 139.52270767] [ 61.82747755 122.76616667] [ 66.34753722 157.38096482] [ 65.32063203 145.03737562] [ 66.1038728 148.64518257] [ 64.52718203 132.68086824]] ['Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female'] ['Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female'] [[ 69.97090362 202.86329303] [ 65.31346154 163.35733152] [ 71.90600544 217.02771464] [ 66.23390404 183.16123246] [ 65.81494282 166.58361051] [ 72.84494786 205.25088915] [ 70.94944345 189.70308882] [ 71.34380763 218.5863364 ] [ 71.05566416 203.13943303] [ 61.92554729 139.90507032] [ 58.52542646 107.79249645] [ 60.34078108 111.07775447] [ 61.43391285 97.05854929] [ 65.47590249 154.4408869 ] [ 60.48444501 121.79779334] [ 62.00031639 117.04835397] [ 59.66765209 114.3868654 ] [ 63.80329944 124.81306876] [ 62.86838051 132.69486866] [ 66.40655275 161.17621832]] Output= ['Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female'] 50 Training Accuracy= 94.0 % Output= ['Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male'] Testing Accuracy= 80.0 % Referensi : https://id.wikipedia.org/wiki/KNN https://www.kaggle.com/shawon10/male-female-detection-by-height-weight-knn","title":"KNN"},{"location":"knn/#pengertian-knnk-nearest-neighbor","text":"Algoritme k-nearest neighbor (k-NN atau KNN) adalah sebuah metode untuk melakukan klasifikasi terhadap objek berdasarkan data pembelajaran yang jaraknya paling dekat dengan objek tersebut. Data pembelajaran diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi data pembelajaran. Sebuah titik pada ruang ini ditandai kelas c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat titk tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Euclidean. Pada fase pembelajaran, algoritme ini hanya melakukan penyimpanan vektor-vektor fitur dan klasifikasi dari data pembelajaran. Pada fase klasifikasi, fitur-fitur yang sama dihitung untuk data test (yang klasifikasinya tidak diketahui). Jarak dari vektor yang baru ini terhadap seluruh vektor data pembelajaran dihitung, dan sejumlah k buah yang paling dekat diambil. Titik yang baru klasifikasinya diprediksikan termasuk pada klasifikasi terbanyak dari titik-titik tersebut. Nilai k yang terbaik untuk algoritme ini tergantung pada data; secara umumnya, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritme nearest neighbor . Ketepatan algoritme k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritme ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur, agar performa klasifikasi menjadi lebih baik.","title":"Pengertian  KNN(k-nearest neighbor)"},{"location":"knn/#terdapat-beberapa-jenis-algoritme-pencarian-tetangga-terdekat-diantaranya","text":"Linear scan Pohon kd Pohon Balltree Pohon metrik Locally-sensitive hashing (LSH) Algoritme k-NN ini memiliki konsistensi yang kuat. Ketika jumlah data mendekati tak hingga, algoritme ini menjamin error rate yang tidak lebih dari dua kali Bayes error rate ( error rate minimum untuk distribusi data tertentu).","title":"Terdapat beberapa jenis algoritme pencarian tetangga terdekat, diantaranya:"},{"location":"knn/#implementasi-knn","text":"Pada data \"Male Female Detection By Height Weight\".","title":"Implementasi KNN"},{"location":"knn/#mengimpor-dan-mengumpulkan-data","text":"data=pd.read_csv('../input/weight-height.csv') data.head(10)import numpy as np from collections import Counter import matplotlib.pyplot as plt import pandas as pd data=pd.read_csv('../input/weight-height.csv') data.head(10)","title":"Mengimpor dan mengumpulkan data"},{"location":"knn/#menghapus-kolom-gender-yang-untuk-menampilkan-data-numerik","text":"df=data columns=['Gender'] df = df.drop(columns, axis=1) df.head(10)","title":"Menghapus kolom \"gender\" yang untuk menampilkan data numerik"},{"location":"knn/#menggabungkan-string-frame-dan-di-tampung-menggunakan-array","text":"Xm=df[0:25] Xf=df[5001:5026] frame=[Xm,Xf] X_train=pd.concat(frame) X_train=np.array(X_train) print(X_train) X_train.shape lbl=data['Gender'] ym=lbl[0:25] yf=lbl[5001:5026] frames=[ym,yf] Y_train=pd.concat(frames) Y_train=np.array(Y_train) print(Y_train) Y_train.shape label=data['Gender'] lb=label[400:410] lc=label[9000:9010] frames=[lb,lc] Y=pd.concat(frames) Y=np.array(Y) print(Y)","title":"Menggabungkan string \"frame\" dan di tampung menggunakan array"},{"location":"knn/#menampikan-akurasi-data-gender","text":"x = data['Height'] y=data['Weight'] male=x[0:25] female=x[5001:5026] wm=y[0:25] wf=y[5001:5026] plt.scatter(male,wm, color='b') plt.scatter(female,wf,color='g') plt.xlabel('Male') plt.ylabel('Female') plt.show()","title":"Menampikan akurasi data \"gender\""},{"location":"knn/#menggabungkan-string-frame-dan-di-tampung-menggunakan-array_1","text":"testx=df[400:410] testy=df[9000:9010] frames=[testx,testy] X=pd.concat(frames) X=np.array(X) print(X) X.shape","title":"Menggabungkan string \"frame\" dan di tampung menggunakan array"},{"location":"knn/#membuat-fungsi-sederhana-dengan-beberapa-argument-secara-anonymous","text":"def predict(input_feature_set, k): distances = [] z=0 for training_feature_set in X_train: group=Y_train[z] #print(\"Group=\",group) #print(\"Training Feature=\",training_feature_set) euclidean_distance = np.linalg.norm(np.array(input_feature_set) - np.array(training_feature_set)) #print(\"Distance=\",euclidean_distance) distances.append([euclidean_distance, group]) z=z+1 #print(z) nearest = sorted(distances)[:k] #print(\"Sorted=\",nearest) votes=[] #votes = [d[1] for d in nearest] for d in nearest: votes.append(d[1]) #print(votes) #prediction = Counter(votes).most_common(1)[0][0] item={} for i in votes: if i in item: item[i]=item[i]+1 else: item[i]=1 #finding most common class m=0 for k in item: if item[k] m: m=item[k] for k in item: if item[k]==m: index=k prediction=index return prediction","title":"Membuat fungsi sederhana dengan beberapa argument secara anonymous"},{"location":"knn/#menampilkan-data-training-gender","text":"#Training toutput=[] for j in X_train: predicted=predict(j,3) toutput.append(predicted) print(\"Output=\",toutput) print(len(toutput))","title":"Menampilkan data training \"Gender\""},{"location":"knn/#menampilkan-training-acuracy","text":"c=0 for p in range(0,50): if toutput[p]==Y_train[p]: c=c+1 result=(c/50.0)*100 print(\"Training Accuracy=\",result,\"%\") Menampilkan prediksi pada data \"Gender\" output=[] for j in X: predicted=predict(j,3) output.append(predicted) print(\"Output=\",output) Menampilkan Testing acuracy c=0 for p in range(0,20): if output[p]==Y[p]: c=c+1 result=(c/20.0)*100 print(\"Testing Accuracy=\",result,\"%\")","title":"Menampilkan training acuracy"},{"location":"knn/#hasil-implementasi","text":"[[ 73.84701702 241.89356318] [ 68.78190405 162.31047252] [ 74.11010539 212.74085556] [ 71.7309784 220.0424703 ] [ 69.88179586 206.34980062] [ 67.25301569 152.21215576] [ 68.78508125 183.9278886 ] [ 68.34851551 167.97111049] [ 67.01894966 175.9294404 ] [ 63.45649398 156.39967639] [ 71.19538228 186.60492556] [ 71.64080512 213.74116949] [ 64.76632913 167.12746107] [ 69.2830701 189.44618139] [ 69.24373223 186.43416802] [ 67.6456197 172.18693006] [ 72.41831663 196.02850633] [ 63.97432572 172.88347021] [ 69.6400599 185.98395757] [ 67.93600485 182.42664801] [ 67.91505019 174.11592908] [ 69.43943987 197.73142161] [ 66.14913196 149.17356601] [ 75.20597361 228.76178062] [ 67.89319634 162.00665185] [ 65.23001251 141.3058226 ] [ 63.36900376 131.04140269] [ 64.47999743 128.17151122] [ 61.79309615 129.78140705] [ 65.96801895 156.80208261] [ 62.85037864 114.96903825] [ 65.65215644 165.08300121] [ 61.89023374 111.67619921] [ 63.67786815 104.15155964] [ 68.10117224 166.57566076] [ 61.79887853 106.23368699] [ 63.37145896 128.11816912] [ 58.89588635 101.68261336] [ 58.4382491 98.19262093] [ 60.80979868 126.91546328] [ 70.12865283 151.25427035] [ 62.25742965 115.79739341] [ 61.73509022 107.86687236] [ 63.05955669 145.58992915] [ 62.28683837 139.52270767] [ 61.82747755 122.76616667] [ 66.34753722 157.38096482] [ 65.32063203 145.03737562] [ 66.1038728 148.64518257] [ 64.52718203 132.68086824]] ['Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female'] ['Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Male' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female' 'Female'] [[ 69.97090362 202.86329303] [ 65.31346154 163.35733152] [ 71.90600544 217.02771464] [ 66.23390404 183.16123246] [ 65.81494282 166.58361051] [ 72.84494786 205.25088915] [ 70.94944345 189.70308882] [ 71.34380763 218.5863364 ] [ 71.05566416 203.13943303] [ 61.92554729 139.90507032] [ 58.52542646 107.79249645] [ 60.34078108 111.07775447] [ 61.43391285 97.05854929] [ 65.47590249 154.4408869 ] [ 60.48444501 121.79779334] [ 62.00031639 117.04835397] [ 59.66765209 114.3868654 ] [ 63.80329944 124.81306876] [ 62.86838051 132.69486866] [ 66.40655275 161.17621832]] Output= ['Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female'] 50 Training Accuracy= 94.0 % Output= ['Male', 'Male', 'Male', 'Male', 'Female', 'Male', 'Male', 'Male', 'Male', 'Female', 'Female', 'Female', 'Female', 'Male', 'Female', 'Female', 'Female', 'Female', 'Female', 'Male'] Testing Accuracy= 80.0 % Referensi : https://id.wikipedia.org/wiki/KNN https://www.kaggle.com/shawon10/male-female-detection-by-height-weight-knn","title":"Hasil Implementasi"}]}